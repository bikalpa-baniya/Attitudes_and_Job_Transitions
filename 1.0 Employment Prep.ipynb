{"cells":[{"cell_type":"markdown","source":["Bikalpa Baniya \n","\n","Spring 2022 Research Group\n","\n","baniya@wisc.edu"],"metadata":{"id":"Battk2AuG3EO"}},{"cell_type":"markdown","source":["**Description of the Code** \n","\n","The following code prepares data from NLYS79 Employment, Education and other relevant demographic variables. The final data will be used to create models on job transitions. \n","\n","The input for the files are in Raw Data folder in the our 2022 Spring Research Group [google drive](https://drive.google.com/drive/folders/1u9yjW4Cf5zZzB0f2GUL6NU-ShHa_k6rt?usp=sharing). The variables currently collected and processed are from the following NLYS79 categories:\n","\n","\n","*   Weekly Array: Employment Status, Hours Worked, Dual Jobs\n","\n","*   Employment Summary Statistics: Hourly Wage, Previous Job Number \n","\n","*   Education Summary Statistics: Highest Grade Completed, Year Highest Grade Completed, Year High School Completed \n","\n","*   Demograhpic Information: Sample ID, Race, Sex, Year of birh\n","\n","\n","**Structure of Code**\n","\n","1.   Loading packages and path\n","2.   Rename Function: There are thousands of variables so we need to create a function to systematically rename all the variables \n","3.   Threshold Remove: This removes any observation that has more than 4 years gap in missing observations \n","4.   Unique Job Tracker: Individuals can hold multiple jobs at the same time so a tracker was used to identify total number of lifetime jobs and weeks where multiple jobs were held\n","5.   Weekly Employment History:  The tracker in 4 is then used create a dataframe with weekly employment History \n","6.   Wage Data: Wage information is added to each year\n","7.   Hours Worked Data: Hours worked data is added to each year\n","8.   Education: The education data is added to each year \n","9.   To weekly: Wage, Hours Worked and Eduation were captured as yearly data in 6,7,8. Here these three information is converted to weekly information\n","10.  Joining: Joinig all the data cleaned above into one dataframe, filtering only the third week of each month, and converting into long format. \n","\n","\n","\n","*Due to the large size of the input files, running the entire code will take about an hour. Instead each intermediary set saves its data into a temp folder and one can simply load this file to run the rest of the code.*"],"metadata":{"id":"aT1GH8nOG5ii"}},{"cell_type":"markdown","metadata":{"id":"H-xxdQt8nQe7"},"source":["#Loading packages and path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNnKVVNhr4bC"},"outputs":[],"source":["#Import\n","import pandas as pd\n","import numpy as np\n","import re\n","import gc\n","import sys\n","import math\n","from datetime import date"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjPNV74757If"},"outputs":[],"source":["#Only run this if this is the first time \n","#pip install cpi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbH9HexJiNGc"},"outputs":[],"source":["#Mounting the drive to ensure data can be uploaded from Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQX2oGaoiPix"},"outputs":[],"source":["#Main Path\n","\n","path_main = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Raw Data'\n","path_main_emp = path_main+'/Employment'\n","path_main_edu = path_main + '/Education/'\n","path_weekly_array = path_main_emp+'/Employment History/Weekly Array/'\n","path_byJob = path_main_emp + '/Employment History/By Jobs/'\n","path_summary_array = path_main_emp+ '/Summary Measures/'"]},{"cell_type":"markdown","metadata":{"id":"SSOQYKWYu0O5"},"source":["#Rename Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AanJiNLFv4N-"},"outputs":[],"source":["#main output: \n","#rename_emp_hist(df, labels, name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqWOc27ciS8L"},"outputs":[],"source":["######Retriving the year variable\n","def get_year_hist(x):\n","    x[\"got_year\"] = -111\n","    if x.year != \"XRND\":\n","      x.got_year = x.year\n","      return x\n","    if x.year == \"XRND\":\n","\n","      #Identify the index of the year \n","      list1 = x.var_title.split()\n","      index=-99\n","      if list1[0] == \"LABOR\" and list1[1]==\"FORCE\":     #If Employment Status\n","        index = 0\n","      elif list1[0] == \"HOURS\" and list1[1]==\"AT\":   #If Hours Worked \n","        index = 0\n","      elif list1[0] == \"JOB\" and list1[1]==\"NUMBER\":    #If Dual Job\n","        index = 1\n","      elif list1[2]==\"OF\" and list1[3]==\"JOB\":    #If Job Date\n","        index = 1\n","      elif list1[2] == \"NO\" and list1[4]==\"GAP\":    #If Within Job gap\n","        index = 2\n","      elif list1[2] == \"GAP\" and list1[4]==\"NOT\":    #If Between Job gap\n","        index = 1\n","      elif list1[0] == \"PREVIOUS\" and list1[1]==\"JOB\":    #If Previous Job number\n","        index = 1\n","\n","\n","      #Getting the year\n","      split_prep2 = re.sub(\"[^0-9]\", \" \", x.var_title)\n","      list2 = split_prep2.split()\n","      x.got_year =  list2[index]  \n","            \n","      return x\n","\n","######Retriving the job# variable\n","def get_job_hist(x):\n","  x[\"got_job\"] = -111\n","  list1 = x.var_title.split()\n","                      \n","  if \"JOB\" in list1:\n","    job_index = list1.index(\"JOB\")\n","    if job_index != len(list1)-1: \n","      x.got_job = re.sub(\"[^0-9]\", \"\", list1[job_index+1])\n","      if x.got_job == \"\":\n","        x.got_job = re.sub(\"[^0-9]\", \"\", list1[job_index+2])\n","        if x.got_job == \"\":          ###Added fix for Previous Job Number\n","          x.got_job = re.sub(\"[^0-9]\", \"\", list1[job_index+6])\n","    else: \n","      x.got_job= \"\"\n","  else:\n","    x.got_job= \"\"\n","  return x\n","\n","\n","######Retriving the week variable\n","def get_week_hist(x):\n","  x[\"got_week\"] = -111\n","  list1 = x.var_title.split()\n","  if \"WEEK\" in list1:\n","    week_index = list1.index(\"WEEK\")\n","    if list1[week_index+1].isdigit():\n","      x.got_week = list1[week_index+1]\n","    else:\n","      x.got_week = \"\"\n","  else: \n","    x.got_week = \"\"\n","  return x\n","\n","\n","######Retriving the gap variable\n","def get_gap_hist(x):\n","  x[\"got_gap\"] = -111\n","  list1 = x.var_title.split()\n","  if \"GAP\" in list1:\n","    gap_index = list1.index(\"GAP\")\n","    x.got_gap = list1[gap_index+1]\n","  else:\n","    x.got_gap= \"\"\n","  return x\n","\n","\n","######Retriving the gap type variable\n","def get_gap_type_hist(x):\n","  x[\"got_gap_type\"] = -111\n","  list1 = x.var_title.split()\n","  if \"GAP\" in list1 and \"NO\" in list1 and \"WORK,\" in list1:\n","    x.got_gap_type = \"WithinJob\"\n","  elif \"GAP\" in list1 and \"NOT\" in list1 and \"WORKING,\" in list1: \n","    x.got_gap_type = \"BetweenJob\"\n","  else: \n","    x.got_gap_type = \"\"\n","  return x\n","\n","######Retriving the Start/Stop type for Emp Hist Array\n","def get_Start_Stop (x):\n","  x[\"got_Start_Stop\"] = -111\n","  list1 = x.var_title.split()\n","  if \"START\" in list1 and \"WEEK\" in list1:\n","    x.got_Start_Stop = \"Start\"\n","  elif \"STOP\" in list1 and \"WEEK\" in list1: \n","    x.got_Start_Stop = \"Stop\"\n","  else: \n","    x.got_Start_Stop = \"\"\n","  return x\n","\n","\n","######Collecting weeks jobs and gaps \n","def rename_collect(x, name):\n","  new_name=\"\"\n","\n","  #has job#,gap#,start/stop & within/bewteen  =  Within job start/stop.   name_year1985_Start_WithinJob_gap1_job1\n","  if x.got_week==\"\" and x.got_job!=\"\" and x.got_gap!=\"\" and x.got_Start_Stop!=\"\" and x.got_gap_type==\"WithinJob\": \n","    new_name = name+\"_year\"+ x.got_year + \"_\"+ x.got_Start_Stop+\"_\" + x.got_gap_type +\"_gap\"+ x.got_gap +\"_job\"+ x.got_job\n","\n","  #has gap#, start/stop & within/bewteen  =  Between job start/stop.      name_year1985_Start_BetweenJob_gap1\n","  elif x.got_week==\"\" and x.got_job==\"\" and x.got_gap!=\"\" and x.got_Start_Stop!=\"\" and x.got_gap_type==\"BetweenJob\": \n","    new_name = name+\"_year\"+ x.got_year + \"_\" +x.got_Start_Stop+\"_\"+ x.got_gap_type + \"_gap\"+ x.got_gap\n","\n","  #has start/stop & Job#  =  Job start/stop Week.                         name_year1985_Start_job1\n","  elif x.got_week==\"\" and x.got_job!=\"\" and x.got_gap==\"\" and x.got_Start_Stop!=\"\" and x.got_gap_type==\"\": \n","    new_name = name+\"_year\"+ x.got_year + \"_\"+ x.got_Start_Stop+\"_job\"+ x.got_job\n","\n","  #has week = Emp_status among other things.                              name_year1985_week94\n","  elif x.got_week!=\"\" and x.got_job==\"\" and x.got_gap==\"\":                 \n","    new_name = name+\"_year\"+ x.got_year +\"_week\"+ x.got_week\n","\n","  elif x.got_week==\"\" and x.got_job!=\"\" and x.got_gap==\"\":                  #has job, like in wage labels and hours worked labels\n","    new_name = name+\"_year\"+ str(x.got_year) +\"_job\"+ str(int(x.got_job))            #The str(int()) to remove 0 from 01\n","\n","  elif x.got_week!=\"\" and x.got_job!=\"\" and x.got_gap==\"\":                  #has week, job# \n","    new_name = name+\"_year\"+ x.got_year +\"_week\"+ x.got_week +\"_job\"+ x.got_job\n","\n","  return new_name\n","\n","##########################################################\n","###############Building the rename function###############\n","##########################################################\n","\n","def rename_emp_hist(df, labels, name):\n","\n","  #######Creating the new name \n","\n","  labels.rename(columns ={\"YEAR\":\"year\", \"VARIABLE TITLE\":\"var_title\"},inplace = True)\n","  labels = labels.apply(get_year_hist, axis = 1 )\n","  labels = labels.apply(get_week_hist, axis = 1 )\n","  labels = labels.apply(get_job_hist, axis = 1 )\n","  labels = labels.apply(get_gap_hist, axis = 1 )\n","  labels = labels.apply(get_gap_type_hist, axis = 1 )\n","  labels = labels.apply(get_Start_Stop, axis = 1)\n","  labels[\"var_name\"] = labels.apply(rename_collect,args = [name], axis = 1 )\n","\n","  \n","  #######Changing the name of the variables\n","\n","  #Manually appending the four identifiers because the label files do not have these variables and \n","  #they are needed while merging\n","  labels2 = pd.DataFrame({\"RNUM\":[\"R0000100\", \"R0173600\", \"R0214700\",\"R0214800\"], \n","                      \"var_name\":[\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"]})\n","  labels = labels.append(labels2, ignore_index = True)\n","\n","  #The actual renaming\n","  #df= df.rename(columns=lambda x: labels[\"var_name\"][labels.index[labels['RNUM']==x].tolist()[0]])\n","  df= df.rename(columns=lambda x: lambda_rename(x, labels))\n","  return df \n","\n","def get_CPS_year_name(RNUM):\n","\n","  CPS_year_hours_worked= [1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993]\n","  CPS_RNUM_hours_worked = [\"R0264300\",\"R0446900\",\"R0702600\",\"R0945700\",\"R1256100\",\"R1650900\",\"R1923500\",\"R2318300\",\"R2526100\",\"R2925100\",\"R3127900\",\"R3523600\",\"R3728600\",\"R4182600\"]\n","  \n","  CPS_year_hourly_wage = [1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994]\n","  CPS_RNUM_hourly_wage = [\"R0047010\",\"R0263710\",\"R0446810\",\"R0702510\",\"R0945610\",\"R1256010\",\"R1650810\",\"R1923410\",\"R2318210\",\"R2526010\",\"R2925010\",\"R3127800\",\"R3523500\",\"R3728500\",\"R4416800\",\"R5079800\"]\n","\n","  if RNUM in CPS_RNUM_hourly_wage:\n","    label = \"CPShry_wage_year\" + str(CPS_year_hourly_wage[CPS_RNUM_hourly_wage.index(RNUM)]) +\"_job0\"\n","  elif RNUM in CPS_RNUM_hours_worked:\n","    label = \"CPShrs_work_year\" + str(CPS_year_hours_worked[CPS_RNUM_hours_worked.index(RNUM)]) +\"_job0\"\n","  else: \n","    label = \"CPS_undefined_job0\"\n","  \n","  #print (label + \" -> \" +RNUM)\n","  return label\n","\n","\n","def lambda_rename(x, labels_df):\n","  if x in labels_df[\"RNUM\"].tolist():\n","    output = labels_df[\"var_name\"][labels_df.index[labels_df['RNUM']==x].tolist()[0]]\n","    if len(output) == 0:              #For hours worked CPS\n","      output = get_CPS_year_name(x)\n","  else:\n","    #\"HOURLY RATE OF PAY CURRENT/MOST RECENT JOB\" removed from hourly wages_labels but not from hourly wages. \n","    output = get_CPS_year_name(x)\n","  return output\n","\n","\n","##########Testing######\n","# ####Loading the test file \n","# var_name_path_test = path_weekly_array +'Var_name_test.csv'\n","# print(var_name_path_test)\n","# var_name_test = pd.read_csv(var_name_path_test)\n","# print(var_name_test)\n","\n","# ####Running the rename code \n","# var_name1 = rename_emp_hist(var_name_test,\"test\")\n","# var_name1[[\"Index\", \"var_title\",\"got_year\",\"got_week\", \"got_job\", \"got_gap\", \"var_name\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oa1pgVQoGCxz"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"MM9g1orJkT2p"},"source":["#Threshold Remove"]},{"cell_type":"markdown","metadata":{"id":"pfpIjJ_tiBMm"},"source":["Main output(s):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zV5f95Fv8Za"},"outputs":[],"source":["#sub_emp_status_thresh"]},{"cell_type":"markdown","metadata":{"id":"SHv6ZOkcDMl_"},"source":["Only run the commented code blocks after this if this is the first time. Otherwise can load this pre saved df "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHt2CucDCwA-"},"outputs":[],"source":["#The commented code blocks after this, only run if first time. Otherwise can load this pre saved df \n","\n","sub_emp_status_thresh_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/sub_emp_status_thresh.csv'\n","sub_emp_status_thresh = pd.read_csv(sub_emp_status_thresh_path)\n","sub_emp_status_thresh.drop('Unnamed: 0', inplace=True, axis=1)\n","\n","\n","sub_emp_status_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/sub_emp_status.csv'\n","sub_emp_status = pd.read_csv(sub_emp_status_path)\n","sub_emp_status.drop('Unnamed: 0', inplace=True, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbEIfXMjkYdi"},"outputs":[],"source":["# #Loading Employment Status variables\n","# path_emp_status = path_weekly_array + \"Employment_status.csv\"\n","# path_emp_status_label = path_weekly_array + \"Employment_status_labels.csv\"\n","# emp_status = pd.read_csv(path_emp_status)\n","# emp_status_label = pd.read_csv(path_emp_status_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIBkbV9fkcEW"},"outputs":[],"source":["# #Renaming the Employment Status variables\n","# emp_status_title = \"emp_status\"\n","# emp_status_renamed = rename_emp_hist(emp_status, emp_status_label,emp_status_title)\n","\n","# #Some errors in NLYS\n","# emp_status_renamed = emp_status_renamed.rename(columns={\"emp_status_year2014_week1812\": \"emp_status_year2014_week1912\"})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HypkOgq1kisR"},"outputs":[],"source":["# def threshold_remove_identify(df, threshold, id_list):\n","#   df = df.sort_index(axis=1)\n","#  # df = df.applymap(lambda x: -1 if x ==-11 else x)\n","#   df[\"counter\"] = 1\n","#   col_list = list(df.columns)\n","#   for i in range((2184-threshold)+1+1):  #1(thresh)+ ___1769___ 1(thresh) + 1, #  (2184-2*threshold)+2+1\n","#     sub_col_list = col_list[i : i+threshold]  #############Fix index\n","#     df[\"sum\"] = df[sub_col_list].sum(axis=1)\n","#     df.loc[df[\"sum\"] == -threshold, 'counter'] = 0\n","#   id_list.append(\"counter\")\n","#   filtered = df[id_list][df.counter == 1]\n","#   output = filtered\n","#   return output\n","\n","# def threshold_remove(df, threshold, id_list):\n","#   df1 = df.applymap(lambda x: -1 if x <=0 and x!=-99 else x)\n","#   identified = threshold_remove_identify(df1, threshold, id_list)\n","#   if \"counter\" in id_list: id_list.remove(\"counter\")\n","#   Filtered_emp_status = (pd.merge(identified,df, on= id_list,how='inner')).drop(columns=[\"counter\"])\n","#   output = Filtered_emp_status\n","#   return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbnKzt1Bkrav"},"outputs":[],"source":["# id_list = [\"ID\",\t\"SAMPLE_ID\",\t\"SAMPLE_RACE\",\t\"SAMPLE_SEX\"]\n","# sub_emp_status = threshold_remove(emp_status_renamed, 208, id_list)\n","# sub_emp_status_thresh = sub_emp_status[id_list] "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m276nF-xCkU6"},"outputs":[],"source":["# with open(sub_emp_status_path, 'w', encoding = 'utf-8-sig') as f:\n","#   sub_emp_status.to_csv(f)\n","\n","# with open(sub_emp_status_thresh_path, 'w', encoding = 'utf-8-sig') as f:\n","#   sub_emp_status_thresh.to_csv(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"teW4LbJVuo3a"},"outputs":[],"source":["# #Cleaning\n","# del [[emp_status, emp_status_label, emp_status_renamed]]\n","# gc.collect()\n","# emp_status = pd.DataFrame()\n","# emp_status_label = pd.DataFrame()\n","# emp_status_renamed = pd.DataFrame()"]},{"cell_type":"markdown","metadata":{"id":"EdCZtHLbi7X3"},"source":["# Creating unique job tracker"]},{"cell_type":"markdown","metadata":{"id":"6MxFZYSNh8kg"},"source":["Main output:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6262dF1awFOk"},"outputs":[],"source":["#Unique_job_num2"]},{"cell_type":"markdown","metadata":{"id":"kOhHAEo4s420"},"source":["Only run the commented code blocks after this if this is the first time. Otherwise can load this pre saved df "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05qzsdy4sXDt"},"outputs":[],"source":["#The next two code blocks after this, only run if first time. Otherwise can load this pre saved df \n","Unique_job_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/Unique_job_num2.csv'\n","# Unique_job_num2 = pd.read_csv(Unique_job_path)\n","# Unique_job_num2.drop('Unnamed: 0', inplace=True, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZWF8EjyBdX6"},"outputs":[],"source":["# path_Previous_job_num = path_summary_array + \"Previous_job_num.csv\"\n","# path_Previous_job_num_label = path_summary_array + \"Previous_job_num_labels.csv\"\n","# Previous_job_num = pd.read_csv(path_Previous_job_num)\n","# Previous_job_num_label = pd.read_csv(path_Previous_job_num_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZWtCnjPDRqP"},"outputs":[],"source":["# #Rename\n","# Previous_job_num_title = \"Previous_job_num\"\n","# Previous_job_num_renamed = rename_emp_hist(Previous_job_num, Previous_job_num_label, Previous_job_num_title)\n","# #Threshold remove\n","# sub_Previous_job_num_renamed = pd.merge(Previous_job_num_renamed, sub_emp_status_thresh ,on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLkEItKfsRNt"},"outputs":[],"source":["# #Assigning a unique number to each job \n","# def create_tracker_shell(df):\n","#   Unique_job_num = df.copy()\n","#   for i in range(1,6):\n","#     new_id = \"Unique_tracker_year1979_job\" + str(i)\n","#     Unique_job_num[new_id] = i\n","\n","#   tracker = 6\n","\n","#   list = Unique_job_num.columns\n","#   id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","#   already_renamed_list = ['Unique_tracker_year1979_job1', 'Unique_tracker_year1979_job2', 'Unique_tracker_year1979_job3', 'Unique_tracker_year1979_job4', 'Unique_tracker_year1979_job5']\n","#   renaming_col = [x for x in list if (x not in id_list) and (x not in already_renamed_list)]\n","\n","#   for col_name in renaming_col:\n","#       Unique_job_num[col_name] = tracker\n","#       new_name = \"Unique_tracker_\" + col_name[-13:]\n","#       Unique_job_num=Unique_job_num.rename(columns={col_name: new_name})\n","#       tracker = tracker + 1\n","#   return Unique_job_num\n","\n","# Unique_job_num = create_tracker_shell(sub_Previous_job_num_renamed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPTmfYdGhZeq"},"outputs":[],"source":["# def create_unique_tracker(x, col_list, col_list_plus, prev):\n","#   x_prev = prev.loc[prev[\"ID\"] == x.ID]\n","\n","#   for col in col_list:\n","#     c_year = col[-9:-5] #current year \n","#     c_job = col[-1:]   #current job \n","\n","#     prev_year = \"Previous_job_num_year\"+ c_year +\"_job\"+ c_job\n","#     job = int(x_prev[prev_year])\n","\n","#     #There are some inconsistencies. So, this is done to ensure two jobs don't point to the same prev job\n","#     job_list_temp = [\"1\",\"2\",\"3\",\"4\"]\n","#     if c_job in job_list_temp: job_list_temp.remove(c_job)\n","#     job_list=[]\n","#     for i in job_list_temp:\n","#       prev_year_j = \"Previous_job_num_year\"+ c_year +\"_job\"+i\n","#       job_list.append(int(x_prev[prev_year_j]))\n","\n","#     if job not in job_list[:int(c_job)-1]:   \n","#       if job >0 and job <5:                 #Computational reasons, only job 1 to 4 considered\n","#         p_year = str(int(c_year)-1)\n","#         p_num = str(job)\n","#         p_var_name = \"Unique_tracker_year\"+p_year+\"_job\"+p_num\n","#         if p_var_name not in col_list_plus:\n","#           p_year = str(int(c_year)-2)\n","#           p_var_name = \"Unique_tracker_year\"+p_year+\"_job\"+p_num\n","        \n","#         p_tracker = x.loc[p_var_name]   #job number in that previous year as listed in unique_tracker..\n","#         x[col] = p_tracker\n","#   return x \n","\n","# id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","# already_renamed_list = ['Unique_tracker_year1979_job1', 'Unique_tracker_year1979_job2', 'Unique_tracker_year1979_job3', 'Unique_tracker_year1979_job4', 'Unique_tracker_year1979_job5']\n","# tracker_col = [x for x in list(Unique_job_num.columns) if (x not in id_list) and (x not in already_renamed_list)]\n","# tracker_col_plus = [x for x in list(Unique_job_num.columns) if (x not in id_list)]\n","\n","# Unique_job_num2 = Unique_job_num.apply(create_unique_tracker, args = [tracker_col, tracker_col_plus, sub_Previous_job_num_renamed], axis = 1 )\n","\n","# #Testing \n","# # previous_job_mini = sub_Previous_job_num_renamed.head(20)\n","# # previous_job_mini.iloc[4,5]=1\n","# # Unique_job_num_mini = Unique_job_num.head(20)\n","# # Unique_job_num2_mini = Unique_job_num_mini.apply(create_unique_tracker, args = [tracker_col, tracker_col_plus, previous_job_mini], axis = 1 )\n","\n","# with open(Unique_job_path, 'w', encoding = 'utf-8-sig') as f:\n","#   Unique_job_num2.to_csv(f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xl3WrQ4vOUT"},"outputs":[],"source":["# #Cleaning\n","# del [[ Previous_job_num_label, sub_Previous_job_num_renamed, Previous_job_num , Unique_job_num]] \n","# gc.collect()\n","# Previous_job_num = pd.DataFrame()\n","# Previous_job_num_label = pd.DataFrame()\n","# sub_Previous_job_num_renamed = pd.DataFrame()\n","# Unique_job_num = pd.DataFrame()"]},{"cell_type":"markdown","metadata":{"id":"cGIt1TMImcpY"},"source":["# Weekly Employment History"]},{"cell_type":"markdown","metadata":{"id":"05OkHrWTiHwb"},"source":["Main output(s):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RK5utnjuiKFx"},"outputs":[],"source":["#emp_hist_main    , with orginial job number \n","#emp_hist_main3   , with tracker number \n"]},{"cell_type":"markdown","metadata":{"id":"Ui5aSbyIg16f"},"source":["Only run the commented code blocks after this if this is the first time. Otherwise can load this pre saved df "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AtKS7yspgiaW"},"outputs":[],"source":["##The commented code blocks after this, only run if first time. Otherwise can load this pre saved df \n","emp_hist_main3_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/emp_hist_main3.csv'\n","emp_hist_main3 = pd.read_csv(emp_hist_main3_path)\n","emp_hist_main3.drop('Unnamed: 0', inplace=True, axis=1)\n","\n","\n","emp_hist_main_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/emp_hist_main.csv'\n","# emp_hist_main = pd.read_csv(emp_hist_main_path)\n","# emp_hist_main.drop('Unnamed: 0', inplace=True, axis=1)\n","\n","\n","sub_dual_jobs_fixed_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/sub_dual_jobs_fixed.csv'\n","# sub_dual_jobs_fixed = pd.read_csv(sub_dual_jobs_fixed_path)\n","# sub_dual_jobs_fixed.drop('Unnamed: 0', inplace=True, axis=1)\n","# sub_dual_jobs_fixed.drop('Unnamed: 0.1', inplace=True, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQ75jptg0ypI"},"outputs":[],"source":["# #Loading Dual Jobs variables\n","# path_dual_jobs = path_weekly_array + \"Dual_jobs.csv\"\n","# path_dual_jobs_label = path_weekly_array + \"Dual_jobs_labels.csv\"\n","# dual_jobs  = pd.read_csv(path_dual_jobs )\n","# dual_jobs_label = pd.read_csv(path_dual_jobs_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xozlWImMS98j"},"outputs":[],"source":["# #Fixing some errors in the column name that stem from the NLYS\n","# if dual_jobs_label['RNUM'][5866]== \"W1382000\" and dual_jobs_label['RNUM'][5960]== \"W1391400\": \n","#   dual_jobs_label.iloc[5866,3] = \"JOB NUMBER 1 (2016) WEEK 2030\"\n","#   dual_jobs_label.iloc[5960,3] = \"JOB NUMBER 2 (2016) WEEK 2030\"\n","# else: \n","#   sys.exit(\"Fix this NLYS error\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_1OLVFn05i6"},"outputs":[],"source":["# #Renaming the Dual variables\n","# dual_jobs_title = \"dual_jobs\"\n","# dual_jobs_renamed = rename_emp_hist(dual_jobs , dual_jobs_label,dual_jobs_title)\n","\n","# #Threshold remove\n","# sub_dual_jobs_renamed = pd.merge(dual_jobs_renamed, sub_emp_status_thresh ,on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3vYyA0H1Acs"},"outputs":[],"source":["# #Fixing some errors in the column name that stem from the NLYS\n","# missing_dua11_week = [\"dual_jobs_year2019_week\"+str(week)+\"_job1\" for week in [2182, 2183, 2184]]  \n","# for col in missing_dua11_week: sub_dual_jobs_renamed[col] = -99\n","\n","# missing_dual2_week = [ \"dual_jobs_year2019_week\"+str(week)+\"_job2\" for week in range(2168,2184+1)]  \n","# for col in missing_dual2_week: sub_dual_jobs_renamed[col] = -99\n","\n","# dual_job3_holes_week = [0,1,2,3,4,5,6,53,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,106,107,110,111,124,133,135,137,138,139,140,161,185,186,187,189,190,191,192,193,228,229,230,232,233,234,235,236,237,238,239,240,241,242,243,244,260,261,262,264,389,390,391,393,394,395,396,398,399,400,401,417,418,419,495,496,499,500,501,531,532,533,534,535,536,537,538,546,547,548,554,555,583,584,585,586,587,588,589,590,591,592,593,594,597,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182, 2183, 2184]\n","# dual_job3_holes_year = [1978,1978,1978,1978,1978,1978,1978,1978,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1979,1980,1980,1980,1980,1980,1980,1980,1980,1980,1980,1980,1981,1981,1981,1981,1981,1981,1981,1981,1981,1982,1982,1982,1982,1982,1982,1982,1982,1982,1982,1982,1982,1982,1982,1982,1982,1982,1982,1983,1983,1985,1985,1985,1985,1985,1985,1985,1985,1985,1985,1985,1985,1985,1986,1987,1987,1987,1987,1987,1988,1988,1988,1988,1988,1988,1988,1988,1988,1988,1988,1988,1988,1989,1989,1989,1989,1989,1989,1989,1989,1989,1989,1989,1989,1989,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2018,2018,2018,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019]\n","# if (len(dual_job3_holes_week)==len(dual_job3_holes_year)):\n","\n","#   missing_dua13_week = [\"dual_jobs_year\"+str(dual_job3_holes_year[i])+\"_week\"+str(dual_job3_holes_week[i])+\"_job3\" for i in range(0,len(dual_job3_holes_week))]  \n","#   minus_99 = [-99 for i in range(0,len(missing_dua13_week))] \n","#   minus_99_collection = [minus_99 for i in range(0,len(sub_dual_jobs_renamed)) ]\n","#   missing_dua13_week_collection = pd.DataFrame(minus_99_collection, columns=missing_dua13_week)\n","#   sub_dual_jobs_fixed=pd.concat([sub_dual_jobs_renamed,missing_dua13_week_collection],axis=1)\n","# else:\n","#   sys.exit(\"The two list are not of equal length\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AL_6sfYvhL7Y"},"outputs":[],"source":["# with open(sub_dual_jobs_fixed_path, 'w', encoding = 'utf-8-sig') as f:\n","#    sub_dual_jobs_fixed.to_csv(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tcifHuS1WOf"},"outputs":[],"source":["# #Cleaning\n","# del [[dual_jobs, dual_jobs_label, dual_jobs_renamed, missing_dua13_week_collection, sub_dual_jobs_renamed]] \n","# gc.collect()\n","# dual_jobs = pd.DataFrame()\n","# dual_jobs_label = pd.DataFrame()\n","# dual_jobs_renamed = pd.DataFrame()\n","# missing_dua13_week_collection = pd.DataFrame()\n","# sub_dual_jobs_renamed = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSDzNogDlfLw"},"outputs":[],"source":["# id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","# emp_hist_main = pd.merge(sub_dual_jobs_fixed, sub_emp_status ,on= id_list ,how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kwz0eorA1UZC"},"outputs":[],"source":["# #Standardizing the name \n","# def rename_correction(df):\n","#   col_list_combined_pre = list(df.columns)\n","#   col_list_combined_post = list()\n","#   for col in col_list_combined_pre:\n","#     seg = col.split(\"_\")\n","    \n","#     if seg[0] != \"emp\" and seg[0] != \"dual\":\n","#       new_name = col \n","#     else:\n","#       zero = \"\"\n","#       #print(len(seg[3]))\n","#       for i in range(0,8-len(seg[3])): zero=zero+\"0\"\n","#       if seg[0] == \"dual\": \n","#        # print(seg[3])\n","#         new_name = \"job_\" + seg[2]+\"_\" + \"week\"+zero +seg[3][4:]+\"_job\"+str(int(seg[4][-1])+1)  \n","#       elif seg[0] == \"emp\":\n","#         #print(seg[3])\n","#         new_name = \"job_\" + seg[2]+\"_\" + \"week\" +zero+seg[3][4:]+\"_job1\"\n","\n","#     col_list_combined_post.append(new_name)\n","\n","#   df.columns = col_list_combined_post\n","#   return df\n","\n","# emp_hist_main = rename_correction(emp_hist_main)\n","\n","# emp_hist_main_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/emp_hist_main.csv'\n","# with open(emp_hist_main_path, 'w', encoding = 'utf-8-sig') as f:\n","#    emp_hist_main.to_csv(f) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RREaQpXMkTq3"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4m8BtIDN-RFv"},"outputs":[],"source":["# emp_hist_main = pd.merge(emp_hist_main, Unique_job_num2 ,on= id_list ,how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZvOiWWm-jbD"},"outputs":[],"source":["# #Cleaning before the next step because it requires a lot of RAM\n","# del [[sub_dual_jobs_fixed, sub_emp_status, Unique_job_num2]] \n","# gc.collect()\n","# sub_dual_jobs_fixed = pd.DataFrame()\n","# sub_emp_status = pd.DataFrame()\n","# Unique_job_num2 = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f65kdBbD-jgM"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YeK9Z9q7BQu-"},"outputs":[],"source":["# emp_hist_test = emp_hist_main.copy()\n","# #emp_hist_main2 = emp_hist_test.head(2)\n","# emp_hist_main2 = emp_hist_test\n","\n","# #Assigning tracker number to each week\n","\n","# def tracker_ordered(x, id_list, job_col, id_job_col):   \n","  \n","#   #Get the right ordering of the job \n","#   uniq_values = sorted(list(set(x[sorted(job_col)])))\n","#   job_values = sorted([y for y in uniq_values if y<999 and y >0])\n","#   job_values_index1 = [list(x[job_col]).index(y) for y in job_values]\n","#   job_values_index2 = sorted(job_values_index1)\n","#   job_values_index_sorted = [job_values[job_values_index1.index(y)] for y in job_values_index2 ] \n","\n","#   #Change each job number to the right ascending order\n","#   for i in range(0,len(job_values_index_sorted)):\n","#     x[job_col] = x[job_col].replace([job_values_index_sorted[i]],str(i+1))          \n","    \n","#   x[job_col] = x[job_col].astype(int)\n","\n","#   return x[id_job_col]\n","\n","# def assign_tracker_number(x, id_list, job_col, tracker_col, int_year):\n","#   for s_year in range(0,2185):                                          #loop through every week\n","#     sub_job_col = [job_col[4*s_year], job_col[4*s_year+1], job_col[4*s_year+2], job_col[4*s_year+3]]         \n","#                                                                         #list of jobs in this week\n","#     survey_code_j1 = x[sub_job_col[0]]                                  \n","#     if survey_code_j1 >=0 and survey_code_j1 <=10:                      #If job1 survey code this, do the same for all 4 jobs\n","#       for column in sub_job_col: x[column] = 1000+ survey_code_j1\n","#     else:                                                               #Else loop through every job in the week\n","\n","#       for job in sub_job_col:\n","#         survey_code = x[job]\n","#         if survey_code >= 100:                                          #If it contains job info do this \n","#           frac, whole = math.modf(survey_code/100)\n","#           if round(frac*100) <5:                                        #Sometime job is listed are more than 4, ignore in this case\n","#             year = int_year[int(whole)]\n","#             job_num = round(frac*100)\n","#             relevant_tracker = \"Unique_tracker_year\" + str(year) +\"_job\" +str(job_num)  #Get the relevant tracker col\n","#             if year < 2019 and year >1978 and job_num> 0 and job_num < 5:\n","#               x[job] = x[relevant_tracker]                                              #Put the unique tracker info\n","#             else:\n","#               print(job + str(survey_code))\n","#               sys.exit(\"Error\")\n","#           else:                                                         #If it contains a >4 job number\n","#             x[job] = 999\n","#         else:                                                           #If it contains a negative value\n","#           x[job] = 0\n","    \n","#   #Making the tracker number ordered, for example from [1,2,4,7,9] to [1,2,3,4,5]\n","#   id_job_col = id_list + job_col\n","#   x = tracker_ordered(x, id_list, job_col, id_job_col)\n","\n","#   #Only reutrns x with id_list and job_list\n","#   return x\n","\n","\n","# id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","# col_list = list(emp_hist_main.columns)\n","# job_col = sorted([x for x in col_list if (x not in id_list) and (x[:3] == \"job\" )])\n","# tracker_col = [x for x in col_list if (x not in id_list) and (x[:14] == \"Unique_tracker\" )]\n","\n","# int_year = [1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1996,1998,2000,2002,2004,2006,2008,2010,2012,2014,2016,2018]\n","# if len(col_list) == len(id_list) + len(tracker_col) +len(job_col):\n","#   emp_hist_main3 = emp_hist_main2.apply(assign_tracker_number, args = [id_list, job_col, tracker_col, int_year], axis = 1 )\n","# else: \n","#   sys.exit(\"Error, maybe in rename correction\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSh0y7Z2KX1l"},"outputs":[],"source":["# emp_hist_main3_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/emp_hist_main3.csv'\n","# with open(emp_hist_main3_path, 'w', encoding = 'utf-8-sig') as f:\n","#    emp_hist_main3.to_csv(f) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Py1pG3VWxCFZ"},"outputs":[],"source":["# #Cleaning \n","# del [[emp_hist_main, emp_hist_main2, emp_hist_test]] \n","# gc.collect()\n","# emp_hist_main = pd.DataFrame()\n","# emp_hist_main2 = pd.DataFrame()\n","# emp_hist_test = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFWszfS59E-D"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"BylwKcsgDnBe"},"source":["# Wage Data"]},{"cell_type":"markdown","metadata":{"id":"mVaA4JMviWww"},"source":["Main output(s):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvxuGO4xiPTC"},"outputs":[],"source":["#sub_wage\n","#sub_CPS_Int_check_renamed "]},{"cell_type":"markdown","metadata":{"id":"XcVlqYwhiPbF"},"source":["Only run the commented code blocks after this if this is the first time. Otherwise can load this pre saved df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRykiH7YiPiq"},"outputs":[],"source":["#The next two code blocks after this, only run if first time. Otherwise can load this pre saved df \n","# sub_wage_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/sub_wage.csv'\n","# sub_wage = pd.read_csv(sub_wage_path)\n","# sub_wage.drop('Unnamed: 0', inplace=True, axis=1)\n","\n","# sub_CPS_Int_check_renamed_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/sub_CPS_Int_check_renamed.csv'\n","# sub_CPS_Int_check_renamed = pd.read_csv(sub_CPS_Int_check_renamed_path)\n","# sub_CPS_Int_check_renamed.drop('Unnamed: 0', inplace=True, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVehr2NCDfPW"},"outputs":[],"source":["# #Loading Hourly Wages variables\n","# path_hourly_wages = path_summary_array + \"Hourly_wages.csv\"\n","# path_hourly_wages_label = path_summary_array+ \"Hourly_wages_labels.csv\"\n","# hourly_wages = pd.read_csv(path_hourly_wages)\n","# hourly_wages_label = pd.read_csv(path_hourly_wages_label)\n","# hourly_wages_label = hourly_wages_label.loc[hourly_wages_label[\"VARIABLE TITLE\"] != \"HOURLY RATE OF PAY CURRENT/MOST RECENT JOB\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ictSj6l6DuLM"},"outputs":[],"source":["# #Renaming the hourly_wage data \n","# hourly_wage_title = \"hourly_wage\"\n","# hourly_wages_renamed = rename_emp_hist(hourly_wages, hourly_wages_label,hourly_wage_title)\n","\n","# #Threshold remove\n","# sub_hourly_wages_renamed = pd.merge(hourly_wages_renamed, sub_emp_status_thresh ,on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZGfvuCY1zKG"},"outputs":[],"source":["# #Loading CPS int check variables\n","# path_CPS_Int_check = path_summary_array + \"CPS_Int_check_sub.csv\" #Taking in ..sub because there are some repeat col in NLYS\n","# path_CPS_Int_check_label = path_summary_array+ \"CPS_Int_check_labels.csv\"\n","# CPS_Int_check = pd.read_csv(path_CPS_Int_check)\n","# CPS_Int_check_label = pd.read_csv(path_CPS_Int_check_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9mhvPFBWPfm"},"outputs":[],"source":["# #Renaming the CPS Int Check data \n","# CPS_Int_check_title = \"CPS_Int_check\"\n","# CPS_Int_check_renamed = rename_emp_hist(CPS_Int_check, CPS_Int_check_label, CPS_Int_check_title)\n","\n","# #Threshold remove\n","# sub_CPS_Int_check_renamed = pd.merge(CPS_Int_check_renamed, sub_emp_status_thresh , on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_DMzRxtkYkV"},"outputs":[],"source":["# sub_wage_total_info = pd.merge( sub_hourly_wages_renamed , sub_CPS_Int_check_renamed,    on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"] , how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"noXO4waxceQP"},"outputs":[],"source":["#print(sorted(list(sub_wage_total_info.columns)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npdnwvZqinir"},"outputs":[],"source":["# sub_wage_total_info['hourly_wage_year1985_job1']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zifzMaWkBj0"},"outputs":[],"source":["# #         \" current/most recent job and wage info for 5 reported jobs in each interview we redefine\n","# # 5 wage variables (w1-w5) that are equal to current job's wage (wagec) if original entry wi is negative (possibly entry error)\n","# # and wagec is not missing and positive*/  \"\n","\n","\n","# def CPS_assign_wage(x , id_list, hourly_wage_columns, CPS_col, CPS_int_check_col, relv_CPS_years):\n","#   for y in relv_CPS_years:\n","#     CPS_year_col = 'CPShry_wage_year' + str(y) + '_job0'\n","#     CPS_wage = x[CPS_year_col]\n","\n","#     if CPS_wage >0 and y != 1993:\n","#       for j in range(1,5):\n","#         hourly_wage_col = 'hourly_wage_year'+str(y)+'_job'+str(j)\n","#         hourly_wage = int(x[hourly_wage_col])\n","#         CPS_int_check_yj_col = 'CPS_Int_check_year'+str(y)+'_job'+str(j)\n","#         CPS_int_check_yj = int(x[CPS_int_check_yj_col])\n","\n","#         if hourly_wage <0 and CPS_int_check_yj == 1: \n","#           x[hourly_wage_col] = CPS_wage\n","\n","#     elif CPS_wage >0 and y == 1993:         #1993 CPS int check only got one job number #1\n","#       hourly_wage  = x['hourly_wage_year1993_job1']\n","#       CPS_int_check_yj = x['CPS_Int_check_year1993_job1']\n","#       if hourly_wage <0 and CPS_int_check_yj == 1:\n","#         x[hourly_wage_col] = CPS_wage\n","\n","#   return x[id_list + hourly_wage_columns]\n","  \n","\n","\n","# id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","# full_col_list = list(sub_wage_total_info.columns)\n","# CPS_col = [x for x in full_col_list if x[0:6]==\"CPShry\"]\n","# CPS_int_check_col = [x for x in full_col_list if x[0:13]==\"CPS_Int_check\"]\n","# hourly_wage_columns = [x for x in full_col_list if x[0:11]==\"hourly_wage\" and int(x[-1])<5]\n","# int_year = [1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1996,1998,2000,2002,2004,2006,2008,2010,2012,2014,2016,2018]\n","# relevant_CPS_year = [1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993]\n","\n","\n","# sub_wage = sub_wage_total_info.apply( CPS_assign_wage , args = [id_list, hourly_wage_columns, CPS_col, CPS_int_check_col, relevant_CPS_year], axis = 1 )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONa2Ek-HkBm9"},"outputs":[],"source":["# #Inflations adjustment \n","# #and\n","# #Reassign wages above the xth percentile to the value of the xth percentile\n","\n","# import cpi\n","# #cpi.update()       \n","\n","# percentile_upper_bound = 99\n","# Infl_ref_year = 2010     #Inflation base year      \n","\n","# #Replace negative values with zeros \n","# sub_wage[hourly_wage_columns] = np.where(sub_wage[hourly_wage_columns] < 0, 0, sub_wage[hourly_wage_columns])\n","# sub_wage[hourly_wage_columns] = sub_wage[hourly_wage_columns].apply(lambda x : cpi.inflate(x, int(x.name[-9:-5]) , to=Infl_ref_year))\n","\n","# #Fix the xth percentile issue\n","# all_values = sub_wage[hourly_wage_columns].values \n","# all_values2 = [item for sublist in all_values for item in sublist if item >0]\n","# up_per_value = np.percentile(all_values2, percentile_upper_bound)\n","# sub_wage[hourly_wage_columns] = np.where(sub_wage[hourly_wage_columns] > up_per_value , up_per_value, sub_wage[hourly_wage_columns])\n","# sub_wage = sub_wage.astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdWpbBCINZAU"},"outputs":[],"source":["# #Remove spikes from wage data because they might be errors \n","# def spike_remove(df, job_num, id_list):\n","#   df = df.applymap(lambda x: -1 if x <0 else x)\n","#   df = df.sort_index(axis=1)\n","\n","#   job_number = str(job_num)\n","#   col_list = list(df.columns)\n","#   sub_col_list = [x for x in col_list if (x in id_list or x[-1]==job_number)]\n","  \n","#   i = 4\n","#   while i +3<= len(sub_col_list):\n","#     sub2_col_list = sub_col_list[i:i+3]\n","#     df[\"temp_value\"] = df.apply(spike_change,args = [sub2_col_list], axis = 1 )\n","#     df.loc[:,sub2_col_list[1]] = df[\"temp_value\"]\n","#     df.drop(columns=['temp_value'])\n","#     i +=1\n","\n","#   output = df.loc[:,sub_col_list]\n","#   return output\n","\n","\n","# def spike_change(row, col_list):\n","#   first = row[col_list[0]]\n","#   second = row[col_list[1]]\n","#   third = row[col_list[2]]\n","#   value_list = [first, second, third]\n","#   if 0 not in value_list and -1 not in value_list:\n","#     change_0_1 =  row[col_list[1]] / row[col_list[0]]\n","#     change_1_2 =  row[col_list[1]] / row[col_list[2]]\n","#     Spike_size = 3\n","#     if (abs(change_0_1) < 0.3333 and abs(change_1_2) < 0.3333) or (abs(change_0_1) > 3 and abs(change_1_2) > 3):      \n","#       output = None \n","#     else:\n","#       output = second\n","#     #More testing is needed, with artificial sample \n","#   else:\n","#     output = second\n","#   return output\n","\n","# def spike_remove_alljobs(df,id_list,total_job):\n","#   wage1 = df[id_list]\n","#   for i in range(1,total_job+1):\n","#     wages = spike_remove(df, i , id_list)\n","#     wage1 = pd.merge(wage1, wages ,on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')\n","#   return wage1\n","\n","# id_list = [\"ID\",\t\"SAMPLE_ID\",\t\"SAMPLE_RACE\",\t\"SAMPLE_SEX\"]\n","# sub_wage = spike_remove_alljobs(sub_wage, id_list, 4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXYrLtRmkA74"},"outputs":[],"source":["# with open(sub_wage_path, 'w', encoding = 'utf-8-sig') as f:\n","#   sub_wage.to_csv(f)\n","\n","# with open(sub_CPS_Int_check_renamed_path, 'w', encoding = 'utf-8-sig') as f:\n","#   sub_CPS_Int_check_renamed.to_csv(f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-D6WTAaTGmw"},"outputs":[],"source":["# #Cleaning\n","# del [[ hourly_wages, hourly_wages_label, hourly_wages_renamed , sub_hourly_wages_renamed, CPS_Int_check, CPS_Int_check_label, CPS_Int_check_renamed, sub_wage_total_info]] \n","# gc.collect()\n","# hourly_wages = pd.DataFrame()\n","# hourly_wages_label = pd.DataFrame()\n","# hourly_wages_renamed = pd.DataFrame()\n","# sub_hourly_wages_renamed = pd.DataFrame()\n","# CPS_Int_check = pd.DataFrame()\n","# CPS_Int_check_label = pd.DataFrame()\n","# CPS_Int_check_renamed = pd.DataFrame()\n","# sub_wage_total_info = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVPtKMKnl8i4"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"QoOT7bIXl-Pw"},"source":["#Hours Worked Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8LKvsXXmBxA"},"outputs":[],"source":["#hours_worked_weekly"]},{"cell_type":"markdown","metadata":{"id":"fM0I-2s-mC_6"},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBB3tztlmDJK"},"outputs":[],"source":["#The next code blocks after this, only run if first time. Otherwise can load this pre saved df \n","hours_worked_weekly_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/hours_worked_weekly.csv'\n","hours_worked_weekly = pd.read_csv(hours_worked_weekly_path)\n","hours_worked_weekly.drop('Unnamed: 0', inplace=True, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUp96PfimDvO"},"outputs":[],"source":["# #Loading Hours worked data\n","# path_hours_worked = path_summary_array + \"Hours_worked.csv\"\n","# path_hours_worked_label = path_summary_array + \"Hours_worked_label.csv\"\n","# hours_worked = pd.read_csv(path_hours_worked)\n","# hours_worked_label = pd.read_csv(path_hours_worked_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ea4FIa9CmFjL"},"outputs":[],"source":["# #Renaming\n","# hours_worked_title = \"hours_worked\"\n","# hours_worked_renamed = rename_emp_hist(hours_worked, hours_worked_label,hours_worked_title)\n","\n","# #Threshold remove\n","# sub_hours_worked_renamed = pd.merge(hours_worked_renamed, sub_emp_status_thresh ,on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygo77oOu4kIG"},"outputs":[],"source":["# #Replacing negative values with zeros\n","\n","# id_list = [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"]\n","# hours_worked_list = [x for x in sorted(list(sub_hours_worked_renamed.columns)) if x not in id_list and int(x[-1])<5]\n","# sub_hours_worked_renamed[hours_worked_list] = np.where(sub_hours_worked_renamed[hours_worked_list] < 0, 0, sub_hours_worked_renamed[hours_worked_list])\n","# sub_hours_worked_renamed = sub_hours_worked_renamed[id_list + hours_worked_list]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bY-A93v4kS9"},"outputs":[],"source":["# #Replacing 99th percentile plus values with the 99th percentile value \n","\n","# percentile_upper_bound = 99\n","# all_values = sub_hours_worked_renamed[hours_worked_list].values \n","# all_values2 = [item for sublist in all_values for item in sublist if item >0]\n","# up_per_value = np.percentile(all_values2, percentile_upper_bound)\n","# sub_hours_worked_renamed[hours_worked_list] = np.where(sub_hours_worked_renamed[hours_worked_list] > up_per_value , up_per_value, sub_hours_worked_renamed[hours_worked_list])\n","# sub_hours_worked_renamed = sub_hours_worked_renamed.astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8d3_e5gc4kVi"},"outputs":[],"source":["# #Fixing CPS stuff\n","\n","# path_CPS_hours_worked = path_summary_array + \"CPS_hours_worked.csv\"\n","# path_CPS_hours_worked_label = path_summary_array + \"CPS_hours_worked_labels.csv\"\n","# CPS_hours_worked = pd.read_csv(path_CPS_hours_worked)\n","# CPS_hours_worked_label = pd.read_csv(path_CPS_hours_worked_label)\n","\n","# CPS_hours_worked_title = \"CPS_hours_worked\"\n","# CPS_hours_worked_renamed = rename_emp_hist(CPS_hours_worked, CPS_hours_worked_label,CPS_hours_worked_title)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJRYSFtvIOt5"},"outputs":[],"source":["# # #         \" current/most recent job and wage info for 5 reported jobs in each interview we redefine\n","# # # 5 wage variables (w1-w5) that are equal to current job's wage (wagec) if original entry wi is negative (possibly entry error)\n","# # # and wagec is not missing and positive*/  \"\n","\n","\n","# def CPS_assign_hrs_worked(x , id_list, hours_worked_columns, CPS_col, CPS_int_check_col, relv_CPS_years):\n","#   for y in relv_CPS_years:\n","#     CPS_year_col = 'CPShrs_work_year' + str(y) + '_job0'\n","#     CPS_hours_worked = x[CPS_year_col]\n","\n","#     if CPS_hours_worked >0 and y != 1993:\n","#       for j in range(1,5):\n","#         hours_worked_col = 'hours_worked_year'+str(y)+'_job'+str(j)\n","#         hours_worked = int(x[hours_worked_col])\n","#         CPS_int_check_yj_col = 'CPS_Int_check_year'+str(y)+'_job'+str(j)\n","#         CPS_int_check_yj = int(x[CPS_int_check_yj_col])\n","\n","#         if hours_worked ==0 and CPS_int_check_yj == 1: \n","#           x[hours_worked_col] = CPS_hours_worked\n","\n","#     elif CPS_hours_worked >0 and y == 1993:         #1993 CPS int check only got one job number #1\n","#       hours_worked  = x['hours_worked_year1993_job1']\n","#       CPS_int_check_yj = x['CPS_Int_check_year1993_job1']\n","#       if x['hours_worked_year1993_job1'] ==0 and CPS_int_check_yj == 1:\n","#         x['hours_worked_year1993_job1'] = CPS_hours_worked\n","\n","#   return x[id_list + hours_worked_columns]\n","  \n","\n","# sub_hours_worked = pd.merge(CPS_hours_worked_renamed, sub_hours_worked_renamed ,on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')\n","# sub_hours_worked = pd.merge(sub_hours_worked,  sub_CPS_Int_check_renamed ,on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')\n","\n","\n","# id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","# full_col_list = list(sub_hours_worked.columns)\n","# CPS_col = [x for x in full_col_list if x[0:6]==\"CPShrs\"] \n","# CPS_int_check_col = [x for x in full_col_list if x[0:13]==\"CPS_Int_check\"]\n","# hours_worked_columns = [x for x in full_col_list if x[0:12]==\"hours_worked\" and int(x[-1])<5]\n","# int_year = [1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1996,1998,2000,2002,2004,2006,2008,2010,2012,2014,2016,2018]\n","# relevant_CPS_year = [1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993]\n","\n","\n","\n","# sub_hours_worked = sub_hours_worked.apply( CPS_assign_hrs_worked , args = [id_list, hours_worked_columns, CPS_col, CPS_int_check_col, relevant_CPS_year], axis = 1 )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5rJH-Q_QQrb"},"outputs":[],"source":["# sub_hours_worked_test = sub_hours_worked.copy(deep=True)\n","# emp_hist_main_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/emp_hist_main.csv'\n","# emp_hist_main = pd.read_csv(emp_hist_main_path)\n","# emp_hist_main.drop('Unnamed: 0', inplace=True, axis=1)\n","\n","# def to_weekly_x(x, id_list, yearly_list, weekly_list, int_year):\n","\n","#   counter = 0\n","#   for yj in weekly_list:\n","#     counter = 0\n","#     year = yj[-18:-14]\n","#     job = yj[-1]\n","#     job_survey_num = x[yj]\n","#     if job_survey_num >99:\n","                 \n","      \n","#       frac, whole = math.modf(job_survey_num/100)\n","#       if round(frac*100) <5:                                       \n","#           year = int_year[int(whole)]\n","#           job_num = round(frac*100)\n","#           relevant_hours_worked = \"hours_worked_year\" + str(year) +\"_job\" +str(job_num)   \n","          \n","#           if year < 2019 and year >1978 and job_num> 0 and job_num < 5:\n","#             hours_worked = x[relevant_hours_worked]\n","#             if hours_worked >0:\n","#               x[yj] = hours_worked\n","#             else: \n","#               counter = counter +1\n","#       else:                                                        \n","#          x[yj] = -5     #If job 5 was listed\n","\n","#   if counter > 0 : print(counter)\n","#   id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","#   return x[id_list + weekly_list] \n","\n","\n","\n","# def to_weekly(sub_hours_worked, unique_tracker_df, emp_hist_list):\n","#   #the name unique_tracker is reminent for previous versions of the code. Please ignore\n","#   new_col_name = [\"hours_worked_year\" + x[8:] for x in  emp_hist_list]  \n","#   unique_tracker_df_copy = unique_tracker_df #.copy().sample(n=1000)\n","#   unique_tracker_df_copy.columns.values[4:] = new_col_name\n","  \n","#   id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","#   weekly_list = sorted(new_col_name)\n","#   yearly_list = sorted([x for x in list(sub_hours_worked.columns) if x not in id_list and x not in  weekly_list])\n","#   # print(id_list)\n","#   # print(weekly_list)\n","#   # print(yearly_list)\n","\n","\n","#   int_year = [1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1996,1998,2000,2002,2004,2006,2008,2010,2012,2014,2016,2018]\n","#   unique_tracker_df_copy[weekly_list] = np.where(unique_tracker_df_copy[weekly_list] <0, 0, unique_tracker_df_copy[weekly_list])\n","#   unique_tracker_df_copy[weekly_list] = np.where(unique_tracker_df_copy[weekly_list] <99, -10, unique_tracker_df_copy[weekly_list])\n","#   sub_hours_worked = pd.merge(sub_hours_worked,  unique_tracker_df_copy ,on= id_list ,how='inner')\n","#   sub_hours_worked_weekly = sub_hours_worked.apply(to_weekly_x , args = [id_list, yearly_list, weekly_list, int_year], axis =1) \n","  \n","#   return sub_hours_worked_weekly \n","\n","# emp_hist_columns = list(emp_hist_main.columns)\n","# emp_hist_columns2 = emp_hist_columns.copy()\n","# id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","# emp_hist_list = [x for x in emp_hist_columns2 if x not in id_list]\n","# # print(emp_hist_list)\n","\n","# hours_worked_weekly = to_weekly(sub_hours_worked_test, emp_hist_main , emp_hist_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0J0sHJ65L7_c"},"outputs":[],"source":["# hours_worked_weekly_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/hours_worked_weekly.csv'\n","# with open(hours_worked_weekly_path, 'w', encoding = 'utf-8-sig') as f:\n","#   hours_worked_weekly.to_csv(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzejzK3kZ4fb"},"outputs":[],"source":[""]},{"cell_type":"code","source":[""],"metadata":{"id":"ZnkNeUgrpWr3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"CSg2Tm96ej0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Education"],"metadata":{"id":"KZlNcsKVekoI"}},{"cell_type":"code","source":["#Loading Education data\n","path_HGC = path_main_edu + \"Highest_grade_completed.csv\"\n","path_HGC_label = path_summary_array + \"Hours_worked_label.csv\"\n","HGC = pd.read_csv(path_HGC)\n","HGC_label = pd.read_csv(path_HGC_label)"],"metadata":{"id":"q-_itbFtem4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SoOgYOTAesUs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#To weekly"],"metadata":{"id":"JoQmO2EapbGr"}},{"cell_type":"code","source":["# sub_wage_copy = sub_wage.copy(deep=True)\n","# emp_hist_main_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/emp_hist_main.csv'\n","# emp_hist_main = pd.read_csv(emp_hist_main_path)\n","# emp_hist_main.drop('Unnamed: 0', inplace=True, axis=1)\n","\n","# def to_weekly_x(x, name, id_list, yearly_list, weekly_list, int_year):\n","\n","#   counter = 0\n","#   for yj in weekly_list:\n","#     counter = 0\n","#     year = yj[-18:-14]\n","#     job = yj[-1]\n","#     job_survey_num = x[yj]\n","#     if job_survey_num >99:\n","                 \n","      \n","#       frac, whole = math.modf(job_survey_num/100)\n","#       if round(frac*100) <5:                                       \n","#           year = int_year[int(whole)]\n","#           job_num = round(frac*100)\n","#           relevant_col = name+\"_year\" + str(year) +\"_job\" +str(job_num)       \n","          \n","#           if year < 2019 and year >1978 and job_num> 0 and job_num < 5:\n","#             hours_worked_or_wage = x[relevant_col]\n","#             if hours_worked_or_wage  >0:\n","#               x[yj] = hours_worked_or_wage \n","#             else: \n","#               counter = counter +1\n","#       else:                                                        \n","#          x[yj] = -5     #If job 5 was listed\n","\n","#   if counter > 0 : print(counter)\n","#   id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","#   return x[id_list + weekly_list] \n","\n","\n","\n","# def to_weekly(name, sub_df, unique_tracker_df, emp_hist_list):\n","#   #the name unique_tracker is reminent for previous versions of the code. Please ignore\n","#   new_col_name = [name +\"_year\"+ x[8:] for x in  emp_hist_list]  \n","#   unique_tracker_df_copy = unique_tracker_df #.copy().sample(n=10)\n","#   unique_tracker_df_copy.columns.values[4:] = new_col_name\n","  \n","#   id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","#   weekly_list = sorted(new_col_name)\n","#   yearly_list = sorted([x for x in list(sub_df.columns) if x not in id_list and x not in  weekly_list])\n","#   # print(id_list)\n","#   # print(weekly_list)\n","#   # print(yearly_list)\n","\n","\n","#   int_year = [1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1996,1998,2000,2002,2004,2006,2008,2010,2012,2014,2016,2018]\n","#   unique_tracker_df_copy[weekly_list] = np.where(unique_tracker_df_copy[weekly_list] <0, 0, unique_tracker_df_copy[weekly_list])\n","#   unique_tracker_df_copy[weekly_list] = np.where(unique_tracker_df_copy[weekly_list] <99, -10, unique_tracker_df_copy[weekly_list])\n","#   sub_df = pd.merge(sub_df,  unique_tracker_df_copy ,on= id_list ,how='inner')\n","#   sub_df_weekly = sub_df.apply(to_weekly_x , args = [name, id_list, yearly_list, weekly_list, int_year], axis =1) \n","  \n","#   return sub_df_weekly \n","\n","# #This part could be inside\n","# emp_hist_columns = list(emp_hist_main.columns)\n","# emp_hist_columns2 = emp_hist_columns.copy()\n","# id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","# emp_hist_list = [x for x in emp_hist_columns2 if x not in id_list]\n","# # print(emp_hist_list)\n","\n","\n","# #Fix the name here and above\n","# hourly_wage_weekly = to_weekly(\"hourly_wage\",sub_wage_copy, emp_hist_main , emp_hist_list)\n","# hourly_wage_weekly\n"],"metadata":{"id":"A_K40Z79pcaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hourly_wage_weekly_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/hourly_wage_weekly.csv'\n","# with open(hourly_wage_weekly_path, 'w', encoding = 'utf-8-sig') as f:\n","#   hourly_wage_weekly.to_csv(f)\n","\n","hourly_wage_weekly = pd.read_csv(hourly_wage_weekly_path)\n","hourly_wage_weekly.drop('Unnamed: 0', inplace=True, axis=1)"],"metadata":{"id":"G_qDI7ioudY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#hourly_wage_weekly.where(hourly_wage_weekly >0).count(axis=1)"],"metadata":{"id":"08aiRjD4ttQM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JoK2XKRQBVJP"},"source":["#Joining"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kAPYdGcQBXHQ"},"outputs":[],"source":["hours_worked_weekly"]},{"cell_type":"code","source":["hourly_wage_weekly"],"metadata":{"id":"55Nq23MpxfCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emp_hist_main3"],"metadata":{"id":"vaIl9YqKxiFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAlmUrMaBXKW"},"outputs":[],"source":["emp_hist_final = pd.merge(hours_worked_weekly, emp_hist_main3 ,on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')\n","emp_hist_final = pd.merge(emp_hist_final, hourly_wage_weekly ,on= [\"ID\", \"SAMPLE_ID\", \"SAMPLE_RACE\",\"SAMPLE_SEX\"],how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82-4o6ySFd7m"},"outputs":[],"source":["#Rename the columns to be more wide_to_long friendly\n","#do wide to long \n","#Generate year\n","#Then K-means after possible some cross validation \n","#Then education"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZErEYVyGTc0"},"outputs":[],"source":["#Cleaning\n","# del [[ hours_worked_weekly, hourly_wage_weekly, emp_hist_main3]] \n","# gc.collect()\n","# hours_worked_weekly = pd.DataFrame()\n","# hourly_wage_weekly = pd.DataFrame()\n","# emp_hist_main3 = pd.DataFrame()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uD5peObpL0yU"},"outputs":[],"source":["col_list = list(emp_hist_final.columns)\n","id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","renaming_list = [x for x in col_list if x not in id_list]\n","new_name_list = id_list\n","a = 0\n","b=0\n","c=0\n","\n","for x in renaming_list:\n","  if x[:11] == \"hourly_wage\": new_name_list.append(\"hourly_wage_job\"+ x[-1]+\"-\"+x[25:29])\n","  if x[:3] == \"job\": new_name_list.append(\"job_tracker_job\"+ x[-1]+\"-\"+x[17:21])\n","  if x[:12] == \"hours_worked\": new_name_list.append(\"hours_worked_job\"+ x[-1]+\"-\"+x[26:30])\n","\n","print(new_name_list)\n","emp_hist_final.columns = new_name_list\n","print(len(list(emp_hist_final.columns)) == len(set(list(emp_hist_final.columns))))\n","emp_hist_final"]},{"cell_type":"code","source":["s_name_1 = ['hourly_wage_job1','hourly_wage_job2','hourly_wage_job3','hourly_wage_job4']\n","s_name_2 = ['job_tracker_job1','job_tracker_job2','job_tracker_job3','job_tracker_job4']\n","s_name_3 = ['hours_worked_job1','hours_worked_job2','hours_worked_job3','hours_worked_job4']\n","s_name = s_name_1 + s_name_2 + s_name_3\n","     \n","reshaped = pd.wide_to_long(emp_hist_final, stubnames = s_name, i=id_list, j='week', sep='-')"],"metadata":{"id":"BmEucgEJEBpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reshaped_temp_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/reshaped_temp.csv'\n","with open(reshaped_temp_path, 'w', encoding = 'utf-8-sig') as f:\n","  reshaped.to_csv(f)\n","   \n","# reshaped_temp = pd.read_csv(reshaped_temp_path)\n","# reshaped_temp.drop('Unnamed: 0', inplace=True, axis=1)"],"metadata":{"id":"erlGEgLdNvNK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vtkQ_Liowjvw"},"source":["Appendix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7see3Pwwlbo"},"outputs":[],"source":["# sub_hours_worked_test = sub_hours_worked.copy(deep=True)\n","# emp_hist_main3 = pd.read_csv(emp_hist_main3_path)\n","# emp_hist_main3.drop('Unnamed: 0', inplace=True, axis=1)\n","\n","# def to_weekly_x(x, id_list, yearly_list, weekly_list):\n","#   print(\"..........\")\n","#   for yj in yearly_list:\n","#     year = yj[-9:-5]\n","#     job = yj[-1]\n","#     if x[yj] >0 or x[yj]<=0:\n","#       rel_col = [y for y in weekly_list if  y[-18:-14] == year  and y[-1]== job]  \n","#       tracker_list = []\n","#       for col in rel_col:\n","#         tracker = x[col]\n","#         if tracker!=0 and tracker < 9999 and tracker not in tracker_list: tracker_list.append(tracker)\n","#       if len(tracker_list) >0:\n","#         print(tracker_list )\n","#         print(year + \"_\"+job)\n","#         print(\"_\")\n","    \n","#   return x \n","\n","\n","# def to_weekly(sub_hours_worked, unique_tracker_df, unique_tracker_list):\n","\n","#   new_col_name = [\"hours_worked_year\" + x[8:] for x in  unique_tracker_list]  \n","#   unique_tracker_df_copy = unique_tracker_df.copy().sample(n=2)\n","#   unique_tracker_df_copy.columns.values[4:] = new_col_name\n","  \n","#   id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","#   weekly_list = new_col_name\n","#   yearly_list = [x for x in list(sub_hours_worked.columns) if x not in id_list and x not in  weekly_list]\n","#   print(id_list)\n","#   print(weekly_list)\n","#   print(yearly_list)\n","\n","#   unique_tracker_df_copy[weekly_list] = np.where(unique_tracker_df_copy[weekly_list] >= 999, 9999, -unique_tracker_df_copy[weekly_list])\n","#   sub_hours_worked = pd.merge(sub_hours_worked,  unique_tracker_df_copy ,on= id_list ,how='inner')\n","#   sub_hours_worked_weekly = sub_hours_worked.apply(to_weekly_x , args = [id_list, yearly_list, weekly_list], axis =1) \n","  \n","#  #return sub_hours_worked #sub_hours_worked_weekly\n","\n","# emp_hist_columns = list(emp_hist_main3.columns)\n","# emp_hist_columns2 = emp_hist_columns.copy()\n","# id_list = ['ID', 'SAMPLE_ID', 'SAMPLE_RACE', 'SAMPLE_SEX']\n","# unique_tracker_list = [x for x in emp_hist_columns2 if x not in id_list]\n","\n","\n","# to_weekly(sub_hours_worked_test, emp_hist_main3 , unique_tracker_list)\n"]},{"cell_type":"markdown","source":["Summary Stat"],"metadata":{"id":"g_x-o-O_xr_o"}},{"cell_type":"code","source":["# emp_hist_final = emp_hist_final_temp.replace(0,np.nan)    \n","# emp_hist_final"],"metadata":{"id":"5DSWMqOqxyv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# job_2 = [x for x in list(emp_hist_final_temp.columns) if x not in id_list and (str(x[-1])==\"2\" and (x in list(sub_wage.columns) or (x in list(sub_hours_worked_renamed)) ) )]\n","# job_2_summary = emp_hist_final_temp[job_2].describe().astype(int)\n","\n","# job_2_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/job_2_summary.csv'\n","# with open(job_2_path, 'w', encoding = 'utf-8-sig') as f:\n","#   job_2_summary.to_csv(f)"],"metadata":{"id":"SCX2k8AOxqbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# job_1 = [x for x in list(emp_hist_final_temp.columns) if x not in id_list and (str(x[-1])==\"1\" and (x in list(sub_wage.columns) or (x in list(sub_hours_worked_renamed)) ) )]\n","# job_1_summary = emp_hist_final_temp[job_1].describe().astype(int)\n","\n","# job_1_path = '/content/drive/MyDrive/Alpha_beta_gamma/Coding/Data Prep/Bikalpa/Temp_df/job_1_summary.csv'\n","# with open(job_1_path, 'w', encoding = 'utf-8-sig') as f:\n","#   job_1_summary.to_csv(f)"],"metadata":{"id":"LZeHK9uUxv4W"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Employment Prep","provenance":[],"collapsed_sections":["H-xxdQt8nQe7","SSOQYKWYu0O5","MM9g1orJkT2p","EdCZtHLbi7X3","cGIt1TMImcpY","BylwKcsgDnBe","QoOT7bIXl-Pw","KZlNcsKVekoI","JoQmO2EapbGr","JoK2XKRQBVJP"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}